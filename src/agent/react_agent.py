"""xSmartDeepResearch ReAct Agent æ ¸å¿ƒå®ç°"""

import json
import time
import asyncio
import re
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime

from openai import AsyncOpenAI

from config import settings, build_system_prompt, FORCE_SUMMARIZE_PROMPT
from src.tools import BaseTool, ToolRegistry
from src.agent.intent_classifier import IntentClassifier
from src.utils.logger import logger
from src.utils.session_manager import SessionManager


@dataclass
class ResearchResult:
    """ç ”ç©¶ç»“æœ"""
    question: str
    answer: str
    prediction: str
    messages: List[Dict[str, str]]
    termination: str
    execution_time: float = 0.0
    iterations: int = 0
    
    def dict(self) -> Dict[str, Any]:
        return {
            "question": self.question,
            "answer": self.answer,
            "prediction": self.prediction,
            "messages": self.messages,
            "termination": self.termination,
            "execution_time": self.execution_time,
            "iterations": self.iterations
        }


class xSmartReactAgent:
    """xSmartDeepResearch ReAct Agent
    
    åŸºäº ReAct (Reasoning + Acting) æ¡†æ¶çš„æ™ºèƒ½ç ”ç©¶ä»£ç†ï¼Œ
    æ”¯æŒå¤šè½®æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯ï¼Œèƒ½å¤Ÿè‡ªä¸»è¿›è¡Œæ·±åº¦ä¿¡æ¯æ£€ç´¢å’Œæ¨ç†ã€‚
    """
    
    # ç‰¹æ®Šæ ‡è®°
    TOOL_CALL_START = "<tool_call>"
    TOOL_CALL_END = "</tool_call>"
    TOOL_RESPONSE_START = "<tool_response>"
    TOOL_RESPONSE_END = "</tool_response>"
    THINK_START = "<think>"
    THINK_END = "</think>"
    ANSWER_START = "<answer>"
    ANSWER_END = "</answer>"
    CODE_START = "<code>"
    CODE_END = "</code>"
    
    def __init__(
        self,
        client: AsyncOpenAI = None,
        model: str = None,
        tools: List[BaseTool] = None,
        max_iterations: int = None,
        max_tokens: int = None,
        temperature: float = None,
        top_p: float = None,
        presence_penalty: float = None,
        timeout_minutes: int = 150,
        classifier_model: str = "gpt-4o-mini"
    ):
        """åˆå§‹åŒ– ReAct Agent"""
        # å®¢æˆ·ç«¯é…ç½®
        self.client = client or AsyncOpenAI(
            api_key=settings.openrouter_key or settings.api_key,
            base_url=settings.api_base,
            timeout=600.0
        )
        self.model = model or settings.model_name
        
        # Agent é…ç½®
        self.max_iterations = max_iterations or settings.max_llm_call_per_run
        self.max_tokens = max_tokens or settings.max_context_tokens
        self.temperature = temperature or settings.temperature
        self.top_p = top_p or settings.top_p
        self.presence_penalty = presence_penalty or settings.presence_penalty
        self.timeout_minutes = timeout_minutes
        
        # æ„å›¾åˆ†ç±»å™¨
        self.classifier = IntentClassifier(self.client, model=classifier_model)
        
        # ä¼šè¯ç®¡ç†å™¨
        self.session_manager = SessionManager()
        self.current_session_id = None
        self.current_project_id = None # ç”¨äºç»‘å®šå½“å‰ Project
        
        # å·¥å…·é…ç½®
        self.tools = {tool.name: tool for tool in (tools or [])}
        
        # Token è®¡æ•°å™¨ (æ‡’åŠ è½½)
        self._tokenizer = None
    
    @property
    def tokenizer(self):
        """æ‡’åŠ è½½ tokenizer"""
        if self._tokenizer is None:
            try:
                import tiktoken
                self._tokenizer = tiktoken.get_encoding("cl100k_base")
            except ImportError:
                self._tokenizer = None
        return self._tokenizer
    
    def register_tool(self, tool: BaseTool) -> None:
        """æ³¨å†Œå·¥å…·
        
        Args:
            tool: å·¥å…·å®ä¾‹
        """
        self.tools[tool.name] = tool
    
    async def run(self, question: str, ground_truth: str = "") -> ResearchResult:
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡ (å¼‚æ­¥ç‰ˆæœ¬)"""
        start_time = time.time()
        
        # ä½¿ç”¨ç”Ÿæˆå™¨è¿è¡Œå¹¶ç´¯ç§¯ç»“æœ
        messages = []
        prediction = ""
        iterations = 0
        termination = "unknown"
        
        async for event in self.stream_run(question):
            event_type = event.get("type")
            
            if event_type == "final_answer":
                prediction = event.get("content", "")
                messages = event.get("messages", [])
                iterations = event.get("iterations", 0)
                termination = event.get("termination", "answer")
            elif event_type == "error":
                prediction = event.get("content", "Error occurred")
                termination = "error"
            elif event_type == "timeout":
                prediction = "Timeout"
                termination = "timeout"
        
        return ResearchResult(
            question=question,
            answer=ground_truth,
            prediction=prediction,
            messages=messages,
            termination=termination,
            execution_time=time.time() - start_time,
            iterations=iterations
        )

    async def stream_run(self, question: str):
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡ (æµå¼ç”Ÿæˆå™¨ç‰ˆæœ¬)
        
        Yields:
            Dict[str, Any]: åŒ…å« type å’Œ content çš„äº‹ä»¶å­—å…¸
        """
        start_time = time.time()
        
        # ğŸŸ¢ æ­¥éª¤ 1: æ„å›¾è¯†åˆ« (åŠ¨æ€äººè®¾æ³¨å…¥)
        yield {"type": "status", "content": "ğŸ” Identifying research intent..."}
        # PERSIST: status
        if self.current_session_id:
             await asyncio.to_thread(self.session_manager.add_message, self.current_session_id, "status", "ğŸ” Identifying research intent...")

        intent = await self.classifier.aclassify(question)
        category = intent.get("category", "general")
        reason = intent.get("reason", "")
        status_msg = f"ğŸ¯ Intent: **{category.upper()}** ({reason})"
        yield {"type": "status", "content": status_msg}
        # PERSIST: status (Create session happens next, so we can't persist this one yet unless we move session creation up. 
        # Actually session creation is the next step. So we should persist this AFTER session creation.)

        # ğŸ”µ æ­¥éª¤ 2: åˆ›å»ºä¼šè¯æŒä¹…åŒ–
        self.current_session_id = await asyncio.to_thread(
            self.session_manager.create_session,
            title=question[:50],  # ç®€å•å–å‰50å­—ç¬¦ä½œä¸ºæ ‡é¢˜
            intent_category=category,
            project_id=self.current_project_id
        )
        # è®°å½•ç”¨æˆ·é—®é¢˜
        await asyncio.to_thread(self.session_manager.add_message, self.current_session_id, "user", question)
        # PERSIST: Delayed status messages
        await asyncio.to_thread(self.session_manager.add_message, self.current_session_id, "status", status_msg)

        # æ„å»ºåˆå§‹æ¶ˆæ¯
        tool_definitions = [tool.get_function_definition() for tool in self.tools.values()]
        system_prompt = build_system_prompt(tool_definitions, category=category)
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ]
        
        iterations = 0
        
        while iterations < self.max_iterations:
            elapsed_minutes = (time.time() - start_time) / 60
            if elapsed_minutes > self.timeout_minutes:
                yield {"type": "timeout", "content": "Research timeout"}
                return

            iterations += 1
            yield {"type": "status", "content": f"Iteration {iterations}...", "iteration": iterations}
            await asyncio.to_thread(self.session_manager.add_message, self.current_session_id, "status", f"Iteration {iterations}...")
            
            # è°ƒç”¨ LLM
            response = await self._call_llm(messages)
            
            if self.TOOL_RESPONSE_START in response:
                pos = response.find(self.TOOL_RESPONSE_START)
                response = response[:pos]
            
            messages.append({"role": "assistant", "content": response.strip()})
            
            # æå–æ€è€ƒè¿‡ç¨‹
            if self.THINK_START in response:
                think_match = re.search(f"{re.escape(self.THINK_START)}(.*?){re.escape(self.THINK_END)}", response, re.DOTALL)
                if think_match:
                    think_content = think_match.group(1).strip()
                else:
                    # å®¹é”™ï¼šå¤„ç†æœªé—­åˆæ ‡ç­¾
                    think_content = response.split(self.THINK_START)[-1].strip()
                    # å¦‚æœåé¢æœ‰å·¥å…·è°ƒç”¨æˆ–ç­”æ¡ˆæ ‡ç­¾ï¼Œæˆªæ–­å®ƒä»¬
                    for tag in [self.TOOL_CALL_START, self.ANSWER_START]:
                        if tag in think_content:
                            think_content = think_content.split(tag)[0].strip()
                
                if think_content:
                    yield {"type": "think", "content": think_content}
                    # è®°å½•æ€è€ƒæ­¥éª¤
                    await asyncio.to_thread(self.session_manager.add_message, self.current_session_id, "thought", think_content)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ
            if self._has_answer(response):
                prediction = self._extract_answer(response)
                
                # è®°å½•æœ€ç»ˆç­”æ¡ˆ
                await asyncio.to_thread(self.session_manager.add_message, self.current_session_id, "answer", prediction)
                
                yield {"type": "answer", "content": prediction}
                yield {
                    "type": "final_answer", 
                    "content": prediction, 
                    "messages": messages, 
                    "iterations": iterations,
                    "termination": "answer"
                }
                return
            
            # æ£€æŸ¥å¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_calls = self._extract_tool_calls(response)
            if tool_calls:
                # 1. äº§ç”Ÿ tool_start äº‹ä»¶
                execution_tasks = []
                tool_names = []
                
                for tc in tool_calls:
                    tool_name = tc.get("name")
                    tool_args = tc.get("arguments", {})
                    
                    yield {
                        "type": "tool_start", 
                        "content": f"Calling tool: {tool_name}", 
                        "tool": tool_name,
                        "arguments": tool_args,
                        "iteration": iterations
                    }
                    
                    logger.info(f"ğŸ”§ Check tool: {tool_name}")
                    
                    if tool_name in self.tools:
                        logger.info(f"ğŸ”§ Executing tool (Parallel): {tool_name}")
                        execution_tasks.append(self.tools[tool_name].call(tool_args))
                        tool_names.append(tool_name)
                    else:
                        # å¯¹äºä¸å­˜åœ¨çš„å·¥å…·ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç›´æ¥è¿”å›é”™è¯¯çš„ mock task
                        async def _not_found_task(t_name=tool_name):
                            return f"[Error] Tool '{t_name}' not found. Available: {list(self.tools.keys())}"
                        execution_tasks.append(_not_found_task())
                        tool_names.append(tool_name)

                # 2. å¹¶è¡Œæ‰§è¡Œ
                if execution_tasks:
                    results = await asyncio.gather(*execution_tasks)
                    
                    # 3. å¤„ç†ç»“æœå¹¶åé¦ˆ
                    combined_tool_outputs = []
                    
                    for i, result in enumerate(results):
                        tool_name = tool_names[i]
                        
                        # è®°å½•å·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
                        await asyncio.to_thread(
                            self.session_manager.add_message,
                            self.current_session_id, 
                            "tool", 
                            f"Call: {tool_name}\nResult Length: {len(str(result))}",
                            metadata={"tool_name": tool_name}
                        )

                        # PERSIST: tool_response
                        await asyncio.to_thread(
                            self.session_manager.add_message,
                            self.current_session_id,
                            "tool_response",
                            result,
                            metadata={"tool_name": tool_name}
                        )

                        yield {
                            "type": "tool_response", 
                            "content": result, 
                            "tool": tool_name,
                            "iteration": iterations
                        }
                        
                        combined_tool_outputs.append(f"Tool '{tool_name}' Output:\n{result}")

                    # å°†æ‰€æœ‰ç»“æœåˆå¹¶ä¸ºä¸€ä¸ª User Message åé¦ˆç»™ LLM
                    # è¿™æ · LLM å¯ä»¥ä¸€æ¬¡æ€§çœ‹åˆ°æ‰€æœ‰å¹¶è¡Œæ‰§è¡Œçš„ç»“æœ
                    full_response_content = "\n\n".join(combined_tool_outputs)
                    messages.append({
                        "role": "user",
                        "content": f"{self.TOOL_RESPONSE_START}\n{full_response_content}\n{self.TOOL_RESPONSE_END}"
                    })
            
            # æ£€æŸ¥ token é™åˆ¶
            token_count = self._count_tokens(messages)
            if token_count > self.max_tokens:
                # å¦‚æœè¿˜æœ‰å¾ˆå¤šæ­¥å¯ä»¥èµ°ï¼Œå°è¯•å‰ªæè€Œä¸æ˜¯ç«‹å³æ€»ç»“
                if iterations < self.max_iterations - 3:
                    logger.info(f"Token count {token_count} exceeds {self.max_tokens}. Pruning context.")
                    messages = self._prune_messages(messages)
                    yield {"type": "status", "content": "Context pruned to save tokens."}
                    await asyncio.to_thread(self.session_manager.add_message, self.current_session_id, "status", "Context pruned to save tokens.")
                else:
                    yield {"type": "status", "content": "Token limit reached, forcing final summary..."}
                    await asyncio.to_thread(self.session_manager.add_message, self.current_session_id, "status", "Token limit reached, forcing final summary...")
                    res = await self._force_summarize(messages, question, "", start_time, iterations)
                    yield {"type": "answer", "content": res.prediction}
                    yield {
                        "type": "final_answer", 
                        "content": res.prediction, 
                        "messages": messages, 
                        "iterations": iterations,
                        "termination": res.termination
                    }
                    return

        yield {"type": "error", "content": "Max iterations exceeded"}
        yield {
            "type": "final_answer", 
            "content": "Max iterations reached without final answer", 
            "messages": messages, 
            "iterations": iterations,
            "termination": "max_iterations_exceeded"
        }

    
    async def _call_llm(self, messages: List[Dict], max_retries: int = 10) -> str:
        """è°ƒç”¨ LLM (å¼‚æ­¥)"""
        base_sleep_time = 1
        
        for attempt in range(max_retries):
            try:
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    stop=[f"\n{self.TOOL_RESPONSE_START}", self.TOOL_RESPONSE_START],
                    temperature=self.temperature,
                    top_p=self.top_p,
                    presence_penalty=self.presence_penalty,
                    max_tokens=10000
                )
                
                content = response.choices[0].message.content
                if content and content.strip():
                    return content.strip()
                    
            except Exception as e:
                logger.error(f"[LLM] Attempt {attempt + 1} failed: {e}")
            
            if attempt < max_retries - 1:
                sleep_time = min(base_sleep_time * (2 ** attempt), 30)
                await asyncio.sleep(sleep_time)
        
        return "LLM call failed after all retries"
    
    def _has_answer(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«æœ€ç»ˆç­”æ¡ˆ"""
        return self.ANSWER_START in content # å®¹é”™ï¼šåªè¦æœ‰å¼€å§‹æ ‡ç­¾å°±è®¤ä¸ºæœ‰ç­”æ¡ˆ
    
    def _extract_answer(self, content: str) -> str:
        """ä»å“åº”å†…å®¹ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
        # å°è¯•åŒ¹é…é—­åˆæ ‡ç­¾
        match = re.search(f"{re.escape(self.ANSWER_START)}(.*?){re.escape(self.ANSWER_END)}", content, re.DOTALL)
        if match:
            return match.group(1).strip()
        
        # å®¹é”™ï¼šå°è¯•åŒ¹é…æœªé—­åˆçš„å¼€å§‹æ ‡ç­¾
        if self.ANSWER_START in content:
            return content.split(self.ANSWER_START)[-1].strip()
            
        return content.strip()
    
    def _has_tool_call(self, content: str) -> bool:
        """æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨"""
        return bool(re.search(r'<tool_call>.*?</tool_call>', content, re.DOTALL)) or \
               bool(re.search(r'<tool_call>.*', content, re.DOTALL)) # å®¹é”™ï¼šå…è®¸æœªé—­åˆæ ‡ç­¾
    
    def _extract_tool_calls(self, content: str) -> List[Dict[str, Any]]:
        """ä»å“åº”ä¸­æå–æ‰€æœ‰å·¥å…·è°ƒç”¨"""
        tool_calls = []
        
        # åŒ¹é…æ‰€æœ‰ <tool_call>...</tool_call> å—
        # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œå¹¶å°½é‡åŒ¹é…é—­åˆæ ‡ç­¾
        # å¦‚æœæœ‰å¤šä¸ªä¸å¸¦æ¢è¡Œç¬¦çš„ tool_callï¼Œæ­£åˆ™å¯èƒ½éœ€è¦è°ƒæ•´ï¼Œä½†é€šå¸¸ LLM ä¼šæ¢è¡Œ
        pattern = r'<tool_call>(.*?)</tool_call>'
        matches = re.finditer(pattern, content, re.DOTALL)
        
        for match in matches:
            tool_call_str = match.group(1).strip()
            # æ¸…ç†å¸¸è§å¹»è§‰
            tool_call_str = tool_call_str.replace("</arg_value>", "").replace("<arg_value>", "")
            tool_call_str = tool_call_str.replace("</tool_code>", "").replace("<tool_code>", "")
            
            try:
                import json5
                try:
                    tool_call_json = json5.loads(tool_call_str)
                except:
                    # ç®€å•ä¿®å¤å°è¯•
                    fixed_str = tool_call_str.strip()
                    if not fixed_str.endswith('}'): fixed_str += '}'
                    tool_call_json = json5.loads(fixed_str)
                
                tool_name = tool_call_json.get("name")
                tool_args = tool_call_json.get("arguments", tool_call_json.get("parameters", {}))
                
                if tool_name:
                    tool_calls.append({
                        "name": tool_name,
                        "arguments": tool_args,
                        "raw": tool_call_str
                    })
            except Exception as e:
                logger.error(f"Failed to parse tool call: {tool_call_str[:50]}... Error: {e}")
                
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ²¡æ‰¾åˆ°é—­åˆçš„ tool_callï¼Œå°è¯•æ‰¾æœªé—­åˆçš„ (é€šå¸¸æ˜¯æµå¼è¾“å‡ºä¸­æ–­æˆ–é”™è¯¯æˆªæ–­)
        if not tool_calls and "<tool_call>" in content:
            # å°è¯•æå–æœ€åä¸€ä¸ªæœªé—­åˆçš„
            last_start = content.rfind("<tool_call>")
            potential_content = content[last_start + 11:].strip()
            if potential_content:
                try:
                    import json5
                    # å°è¯•è¡¥å…¨å¹¶è§£æ
                    if not potential_content.endswith('}'): potential_content += '}'
                    tool_call_json = json5.loads(potential_content)
                    if tool_call_json.get("name"):
                        tool_calls.append({
                            "name": tool_call_json.get("name"),
                            "arguments": tool_call_json.get("arguments", {}),
                            "raw": potential_content
                        })
                except: pass

        # æ£€æŸ¥ PythonInterpreter çš„ code å¿«æ·æ–¹å¼
        # å…¶å® xSmart çš„ PythonInterpreter å¹¶ä¸æ€»æ˜¯ç”¨ <tool_call>ï¼Œæœ‰æ—¶ç”¨ <code>
        # è¿™é‡Œä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œè¿˜æ˜¯ä¿ç•™ _execute_tool_call é‡Œå¯¹ PythonInterpreter çš„ç‰¹æ®Šé€»è¾‘å—ï¼Ÿ
        # ä¸ï¼Œ_execute_tool_call å³å°†è¢«åºŸå¼ƒã€‚æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œå¤„ç† <code> å—ã€‚
        if self.CODE_START in content and self.CODE_END in content:
             # å¦‚æœå·²ç»é€šè¿‡ tool_call è§£æå‡ºäº† PythonInterpreter ä¸”æœ‰ code å‚æ•°ï¼Œåˆ™ä¸ç”¨é‡å¤
             # å¦‚æœæ²¡æœ‰ï¼Œåˆ™æ·»åŠ ä¸€ä¸ªéšå¼çš„ PythonInterpreter è°ƒç”¨
             has_pi = any(tc['name'] == 'PythonInterpreter' for tc in tool_calls)
             if not has_pi:
                 code_match = re.search(f"{re.escape(self.CODE_START)}(.*?){re.escape(self.CODE_END)}", content, re.DOTALL)
                 if code_match:
                     code_content = code_match.group(1).strip()
                     tool_calls.append({
                         "name": "PythonInterpreter",
                         "arguments": code_content, # PythonInterpreter tool æ¥å— string æˆ– dict
                         "raw": code_content
                     })
        
        return tool_calls
    
    def _count_tokens(self, messages: List[Dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯çš„ token æ•°
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            token æ•°é‡
        """
        if self.tokenizer is None:
            # ç²—ç•¥ä¼°è®¡ï¼š4ä¸ªå­—ç¬¦çº¦ç­‰äº1ä¸ªtoken
            total_chars = sum(len(m.get("content", "")) for m in messages)
            return total_chars // 4
        
        full_text = "\n".join(m.get("content", "") for m in messages)
        tokens = self.tokenizer.encode(full_text)
        return len(tokens)
    
    async def _force_summarize(
        self, 
        messages: List[Dict],
        question: str,
        ground_truth: str,
        start_time: float,
        iterations: int
    ) -> ResearchResult:
        """å¼ºåˆ¶æ€»ç»“ï¼ˆtoken è¶…é™æ—¶ä½¿ç”¨ï¼‰"""
        # æ·»åŠ å¼ºåˆ¶æ€»ç»“æç¤º
        messages[-1]["content"] = FORCE_SUMMARIZE_PROMPT
        
        # å†æ¬¡è°ƒç”¨ LLM
        response = await self._call_llm(messages)
        messages.append({"role": "assistant", "content": response.strip()})
        
        if self._has_answer(response):
            prediction = self._extract_answer(response)
            termination = "token_limit_forced_answer"
        else:
            prediction = response
            termination = "token_limit_format_error"
        
        return ResearchResult(
            question=question,
            answer=ground_truth,
            prediction=prediction,
            messages=messages,
            termination=termination,
            execution_time=time.time() - start_time,
            iterations=iterations
        )

    def _prune_messages(self, messages: List[Dict]) -> List[Dict]:
        """å‰ªææ¶ˆæ¯å†å²ï¼Œä¿ç•™æ ¸å¿ƒä¸Šä¸‹æ–‡"""
        if len(messages) <= 8:
            return messages
            
        # 1. ä¿ç•™ System Prompt å’ŒåŸå§‹ User Question
        # æ³¨æ„ï¼šæœ‰æ—¶å€™ç¬¬ä¸€ä¸ªæ¶ˆæ¯ä¸æ˜¯ systemï¼Œæˆ–è€…ç¬¬äºŒä¸ªä¸æ˜¯ userï¼Œä½†è¿™é‡Œåšä¸€èˆ¬æ€§å‡è®¾
        kept_head = messages[:2]
        
        # 2. ä¿ç•™æœ€è¿‘çš„ 3 æ¬¡äº¤äº’ (Assistant + User å…± 6 æ¡æ¶ˆæ¯)
        kept_tail = messages[-6:]
        
        # 3. æ„é€ å‰ªææç¤º
        pruned_notice = {
            "role": "system", 
            "content": f"[System Note: Earlier conversation turns have been removed to save tokens. Current token usage: {self._count_tokens(kept_head + kept_tail)}]"
        }
        
        return kept_head + [pruned_notice] + kept_tail
